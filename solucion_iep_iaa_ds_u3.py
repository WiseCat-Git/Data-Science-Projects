# -*- coding: utf-8 -*-
"""Solucion_IEP-IAA-DS-U3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c99ZfVPBCvzF2wsZZQgoJplsZwse_vii

# **Posibles enfoques de solución**

Para resolver este ejercicio, y desde el enfoque de Ciencias de Datos quisiera mencionar o abordar la solucion desde las posibles aristas o angulos posibles, estos son:

1. PCA (Análisis de Componentes Principales)

  Método lineal basado en la varianza de los datos.
  Útil para datos con alta correlación entre variables.
  Se puede usar para compresión de datos y mejora del rendimiento en modelos de ML.

2. t-SNE (t-Distributed Stochastic Neighbor Embedding)

  Método no lineal basado en la distribución de probabilidad de t-Student.
  Más útil para visualizar datos en espacios de baja dimensión, conservando la estructura local.
  Puede ser computacionalmente costoso para grandes volúmenes de datos.

3. UMAP (Uniform Manifold Approximation and Projection)

  Alternativa moderna a t-SNE, más rápida y escalable.
  Mantiene mejor la estructura global y local de los datos.
  Se usa en combinación con PCA para mejorar la velocidad y rendimiento.

4. Autoencoders (Redes Neuronales)

  Un método basado en redes neuronales para reducción de dimensionalidad.
  Puede capturar relaciones complejas en los datos.
  Ideal para datasets de imágenes como MNIST.

Para seguir lo que requiere el enunciado usare PCA y t-SNE, adicionalmente mencionare como combinar PCA con t-SNE para optimizar rendimiento, y comparare sus resultados en el dataset Iris y MNIST.

**Paso 1.**

Cargamos y exploramos el dataset, importamos las librerias necesarias realizando un analisis exploratorio y visualizamos las relaciones entre las variables presentes.
"""

# Importar librerías necesarias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Cargar el dataset Iris
iris = load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)
data['species'] = iris.target

# Mostrar las primeras filas del dataset
print(" Primeras filas del dataset:")
display(data.head())

# Distribución de las variables numéricas
plt.figure(figsize=(10,6))
sns.pairplot(data, hue="species", palette="coolwarm")
plt.title("Distribución de las características del dataset Iris")
plt.show()

"""Procedemos a aplicar PCA para reducir a 2 componentes ¿Como lo realizaremos?

1. Estandarizamos los datos
2. Aplicamos PCA para obtener 2 componentes
3. Visualizamos la varianza explicada
4. Graficamos los datos transformados en 2D
"""

# Estandarización de los datos
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data.drop(columns=['species']))

# Aplicamos PCA con 2 componentes
pca = PCA(n_components=2)
pca_result = pca.fit_transform(data_scaled)

# Convertimos a DataFrame
pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])
pca_df['species'] = data['species']

# Visualizar la varianza explicada
print("Varianza explicada por cada componente principal:")
print(pca.explained_variance_ratio_)

# Graficar los datos en el nuevo espacio 2D
plt.figure(figsize=(8,6))
sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=pca_df['species'], palette="coolwarm")
plt.title("PCA: Representación en 2D del dataset Iris")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.show()

"""**Que Analisis obtenemos?**

Hemos reducido las 4 dimensiones a 2 con minima perdida de informacion mediante PCA.

La visualizacion muestra grupos diferenciados para cada especie.

# **Ejercicio 2: PCA y t-SNE en el dataset MNIST**
"""

# Importar librerías necesarias
from sklearn.datasets import fetch_openml

# Cargar el dataset MNIST
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data / 255.0  # Normalización
y = mnist.target.astype(int)  # Convertir etiquetas a enteros

# Mostrar dimensiones del dataset
print(f"Dimensiones de X: {X.shape}")
print(f"Dimensiones de y: {y.shape}")

"""Ahora ejecutamos PCA en MNIST"""

# Aplicar PCA con 2 componentes
pca_mnist = PCA(n_components=2)
X_pca = pca_mnist.fit_transform(X)

# Graficar PCA en 2D
plt.figure(figsize=(10,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap="jet", alpha=0.5)
plt.colorbar(label="Dígitos")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA en MNIST (2 componentes)")
plt.show()

"""**Que analisis podemos obtener?**

Lo primero que observamos es que PCA no separa bien los dígitos, lo que nos indica que no esta capturando bien las diferencias en los datos.

Procedemos entonces a usar t-SNE ya que podria ser mas efectivo para este caso puntual.
"""

# Importar TSNE
from sklearn.manifold import TSNE

# Aplicar t-SNE con 2 componentes
tsne = TSNE(n_components=2, random_state=0, perplexity=30)
X_tsne = tsne.fit_transform(X[:5000])  # Reducimos muestra para rapidez

# Graficar t-SNE en 2D
plt.figure(figsize=(10,6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y[:5000], cmap="jet", alpha=0.5)
plt.colorbar(label="Dígitos")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.title("t-SNE en MNIST (2 componentes)")
plt.show()

"""Notese que t-SNE separa mejor los grupos de dígitos, mostrando una estructura más clara.

Tambien cuenta con mayor capacidad para capturar relaciones no lineales en los datos.

**Combinamos PCA + t-SNE para optimizar**
"""

# Aplicar PCA con 10 componentes antes de t-SNE
pca_10 = PCA(n_components=10)
X_pca_10 = pca_10.fit_transform(X)

# Aplicar t-SNE después de PCA
tsne_pca = TSNE(n_components=2, random_state=0, perplexity=30)
X_tsne_pca = tsne_pca.fit_transform(X_pca_10[:5000])

# Graficar
plt.figure(figsize=(10,6))
plt.scatter(X_tsne_pca[:, 0], X_tsne_pca[:, 1], c=y[:5000], cmap="jet", alpha=0.5)
plt.colorbar(label="Dígitos")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.title("PCA + t-SNE en MNIST (2 componentes)")
plt.show()

"""Con ayuda de PCA hemos reducido las dimensiones de 784 a 10, acelerando t-SNE

PCA + t-SNE es una combinacion poderosa para visualizacion de alta dimensionalidad

Importante resaltar que, PCA funciona bien en datos lineales como Iris, t-SNE funciona mejor en datos no lineales como MNIST, una vez combinamos PCA + t-SNE mejoramos la velocidad y calidad de los resultados, pero, como y porque?

Bien, t-SNE es un algoritmo bastante potente para la reduccion de dimensionalidad, pero tiene una complejidad computacional alta. En su implementacion standard, t-SNE tiene una complejidad de O(N^2) debido a que calcula las similitudes entre todos los pares de puntos en el dataset.

Ahora, para datasets grandes como MNIST(70,000 imágenes de 784 pixeles), el tiempo de ejecución puede ser prohibitivo.

Al reducir primero con PCA a 10-50 dimensiones, se mantiene suficiente información para t-SNE sin comprometer demasiado la calidad.

Otro punto importante es la reducción de la complejidad, sin PCA, t-SNE opera sonre 784 dimensiones, lo que implica un coste computacional alto, con PCA reducimos a 10-50 dimensiones disminuyendo la cantidad de cálculos que t-SNE necesita realizar, mejorando así su eficiencia.

Aunque t-SNE es efectivo para capturar relaciones no lineales, tiene problemas con el ruido y la redundancia de datos de alta dimensionalidad.

1. PCA elimina la redundancia, filtra las dimensiones que aportan menos información, mejorando asi la estabilidad y calidad de t-SNE evitando que el algoritmo se vea afectado por ruido.

Unos de los beneficios en la representación de datos es que PCA elimina las dimensiones irrelevantes, permitiendo que t-SNE se enfoque en patrones significativos.

**Demostración de comparación en tiempo de ejecución.**

De esta comparación podemos esperar que se demuestren dos cosas, el tiempo que toma t-SNE sin PCA, y el tiempo que toma t-SNE con PCA (50 Comparaciones), en donde se reduce el tiempo a la mitad o menos, o que MNIST ya este preprocesado con solo información relevante y en cuyo caso PCA + tSNE solo agrega un paso extra que no aporta eficiencia computacional.
"""

import time
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import fetch_openml

# Cargar MNIST
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data / 255.0
y = mnist.target.astype(int)

# Tomamos una muestra de 5000 datos
X_sample = X[:5000]
y_sample = y[:5000]

# t-SNE sin PCA
start_tsne = time.time()
tsne = TSNE(n_components=2, perplexity=30, random_state=0)
X_tsne = tsne.fit_transform(X_sample)
end_tsne = time.time()
print(f"⏱️ t-SNE sin PCA: {end_tsne - start_tsne:.2f} segundos")

# t-SNE con PCA (20 componentes)
start_pca_tsne = time.time()
pca = PCA(n_components=20, svd_solver='randomized', random_state=0)
X_pca = pca.fit_transform(X_sample)
tsne_pca = TSNE(n_components=2, perplexity=50, random_state=0)  # Se ajusta perplexity
X_tsne_pca = tsne_pca.fit_transform(X_pca)
end_pca_tsne = time.time()
print(f"⏱️ t-SNE con PCA (20 comp.): {end_pca_tsne - start_pca_tsne:.2f} segundos")

"""**El ejercicio demuestra que:**

1. En este caso, PCA no ayuda a reducir la complejidad, sino que agrega un paso extra.

2. Los datos ya contienen suficiente información relevante en el espacio original, t-SNE puede trabajar sin necesidad de reducción previa.

La eficiencia de un algoritmo no solo se mide en términos de velocidad, sino también en precisión, consumo de memoria y escalabilidad. Para garantizar que un algoritmo sea eficiente y encontrar oportunidades de optimización de manera justificable, podemos seguir una serie de buenas prácticas estructuradas.

✅ Métricas de tiempo de ejecución

**timeit** en Python: Medir el tiempo exacto que tarda cada proceso.
%%time en Jupyter/Colab: Útil para medir tiempos en celdas individuales.
Comparación de tiempos con diferentes configuraciones: Ejecutar el algoritmo con y sin optimización.
✅ Uso de memoria y recursos computacionales

**memory_profiler**: Evalúa cuánta memoria consume un proceso.
psutil: Permite monitorear el consumo de CPU/RAM.

✅ Prueba en subconjuntos de datos de diferentes tamaños

Aplicar el modelo a **10%, 50%, 100**% de los datos y medir cómo cambian el tiempo de ejecución y la precisión.
✅ Medición de la complejidad computacional

Algoritmos con tiempo de ejecución en O(n), O(n log n), O(n²), etc.
Los algoritmos cuadráticos o exponenciales deben optimizarse para datasets grandes.

Las mejoras deben validarse no solo con tiempos, sino también con visualizaciones para confirmar que la calidad de los resultados no se degrade.

✅ Ejemplo: Antes y después de aplicar PCA a t-SNE:

```
# This is formatted as code
  import matplotlib.pyplot as plt
  import seaborn as sns

  def plot_embedding(X, y, title=""):
      plt.figure(figsize=(8,6))
      sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, palette="Set1", alpha=0.7)
      plt.title(title)
      plt.show()
```

Si PCA degrada la separación de clases en t-SNE, quizás no es la mejor optimización.
"""