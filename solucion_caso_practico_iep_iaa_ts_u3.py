# -*- coding: utf-8 -*-
"""Solucion_Caso_Practico_IEP_IAA_TS_u3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14bmMkH1AkkmGiSPL3N7E583FH8fc8RN-

## Reflexiones clave previas a la implementación de modelos de Deep Learning (RNN, LSTM, GRU)

Antes de ejecutar los modelos, considera las siguientes preguntas críticas para guiar el análisis y justificar tus decisiones:

1. **¿Qué tipo de dependencias temporales esperas capturar?**  
   ¿Hay patrones a corto, medio o largo plazo? Esto influirá directamente en la elección del modelo:  
   - RNN es más simple, pero puede fallar en secuencias largas.  
   - LSTM o GRU son más adecuados para dependencias prolongadas.

2. **¿Qué impacto pueden tener las variables exógenas como hora, día de la semana o festivos en la predicción?**  
   ¿Es posible que una gran parte de la variabilidad se deba a patrones calendario o de estacionalidad horaria? Incorporar estas variables puede mejorar significativamente el rendimiento del modelo.

3. **¿Cuál será tu línea base de comparación y qué esperas superar?**  
   ¿El modelo de regresión lineal ya logra un rendimiento aceptable? ¿Qué ventaja tangible debe ofrecer mi red neuronal para justificar su complejidad?

4. **¿Tienes evidencia visual o estadística de estacionalidad, tendencia o ruido en los datos?**  
   Confirmar la presencia de patrones recurrentes permite validar el uso de modelos secuenciales y seleccionar la ventana de entrada adecuada.

5. **¿Estás controlando el sobreajuste?**  
   ¿Cuentas con suficientes datos de entrenamiento para evitar el overfitting?  
   Considera aplicar regularización (dropout), early stopping y dividir adecuadamente los conjuntos de entrenamiento y validación.

---

Estas reflexiones me permitirán interpretar mejor los resultados y justificar mis decisiones de modelado.
"""

# 1. Carga y visualización inicial del dataset PJME_hourly

import pandas as pd
import matplotlib.pyplot as plt

# Cargar el dataset desde tu Google Drive
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_TS_u3/anexo_caso3_ejercicio1/PJME_hourly.csv')

# Verificamos las primeras filas
print("Primeras filas del dataset:")
print(df.head())

# Convertimos la columna de fecha a datetime y establecemos como índice
df['Datetime'] = pd.to_datetime(df['Datetime'])
df = df.set_index('Datetime')
df = df.rename(columns={'PJME_MW': 'value'})

# Graficamos la serie temporal completa
plt.figure(figsize=(14, 5))
plt.plot(df.index, df['value'], linewidth=0.7)
plt.title('Consumo de Energía PJME (MW) - Serie Temporal Completa')
plt.xlabel('Fecha')
plt.ylabel('Consumo (MW)')
plt.grid(True)
plt.show()

# 2. Crear retrasos temporales (lags) y features calendario

def create_time_delay_embedding(data, horizon=1, final_lag=24, column="value"):
    df_delay = pd.DataFrame()
    df_delay["y"] = data[column].copy()
    for i in range(horizon, final_lag + 1):
        df_delay[f"lag_{i}"] = data[column].shift(i)
    df_delay.index = data.index
    return df_delay.dropna()

# Generamos 24 lags y eliminamos valores nulos
final_lag = 24
horizon = 1
df_lags = create_time_delay_embedding(df, horizon=horizon, final_lag=final_lag)

# Añadimos variables de calendario: hora, día, mes, día de la semana
df_lags["hour"] = df_lags.index.hour
df_lags["day"] = df_lags.index.day
df_lags["month"] = df_lags.index.month
df_lags["day_of_week"] = df_lags.index.dayofweek

# Asignar para que el modelo base lo reconozca
df_features = df_lags

# Mostramos los primeros registros con lags y variables temporales
print("Estructura del dataset con lags y features de calendario:")
print(df_features.head())

# 3. Preparación del dataset: train/val/test, escalado y feriados

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import holidays

# Etiquetamos si el día es feriado (EE.UU.)
us_holidays = holidays.US()
df_lags["is_holiday"] = df_lags.index.to_series().apply(lambda x: int(x.normalize() in us_holidays))

# Separar características y etiqueta (y)
X = df_lags.drop(columns=["y"])
y = df_lags[["y"]]

# Dividir en train, val y test (80% train, 10% val, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)

# Escalar los datos con MinMaxScaler
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = scaler_y.fit_transform(y_train)
y_val_scaled = scaler_y.transform(y_val)
y_test_scaled = scaler_y.transform(y_test)

print("Tamaños de los conjuntos:")
print(f"Train: {X_train_scaled.shape}")
print(f"Val: {X_val_scaled.shape}")
print(f"Test: {X_test_scaled.shape}")

# 4. Convertir a tensores de PyTorch y crear DataLoaders

import torch
from torch.utils.data import TensorDataset, DataLoader

# Hiperparámetro
batch_size = 64

# Convertir arrays a tensores
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)

X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)

# Crear datasets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataset_one = TensorDataset(X_test_tensor, y_test_tensor)  # para batch_size=1

# Crear dataloaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
test_loader_one = DataLoader(test_dataset_one, batch_size=1, shuffle=False, drop_last=True)

# 5. Definición de la arquitectura RNN

import torch.nn as nn

class RNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):
        super(RNNModel, self).__init__()

        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim

        # Definir la capa RNN
        self.rnn = nn.RNN(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=layer_dim,
            batch_first=True,
            dropout=dropout_prob
        )

        # Capa totalmente conectada para la salida
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Inicializar el estado oculto
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()

        # Propagación hacia adelante
        out, _ = self.rnn(x, h0.detach())

        # Tomar la última salida temporal
        out = out[:, -1, :]

        # Pasar por la capa fully-connected
        out = self.fc(out)
        return out

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

class Optimization:
    def __init__(self, model, loss_fn, optimizer):
        self.model = model
        self.loss_fn = loss_fn
        self.optimizer = optimizer
        self.train_losses = []
        self.val_losses = []

    def train_step(self, x, y):
        self.model.train()
        yhat = self.model(x)
        loss = self.loss_fn(y, yhat)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        return loss.item()

    def train(self, train_loader, val_loader, batch_size=64, n_epochs=20, n_features=1):
        for epoch in range(1, n_epochs + 1):
            batch_losses = []
            for x_batch, y_batch in train_loader:
                x_batch = x_batch.view([batch_size, -1, n_features])
                loss = self.train_step(x_batch, y_batch)
                batch_losses.append(loss)
            train_loss = np.mean(batch_losses)
            self.train_losses.append(train_loss)

            # Validación
            with torch.no_grad():
                val_batch_losses = []
                for x_val, y_val in val_loader:
                    x_val = x_val.view([batch_size, -1, n_features])
                    yhat = self.model(x_val)
                    val_loss = self.loss_fn(y_val, yhat).item()
                    val_batch_losses.append(val_loss)
                val_loss = np.mean(val_batch_losses)
                self.val_losses.append(val_loss)

            if (epoch <= 5) or (epoch % 10 == 0):
                print(f"[{epoch}/{n_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    def evaluate(self, test_loader, batch_size=1, n_features=1):
        self.model.eval()
        predictions = []
        values = []
        with torch.no_grad():
            for x_test, y_test in test_loader:
                x_test = x_test.view([batch_size, -1, n_features])
                yhat = self.model(x_test)
                predictions.append(yhat.cpu().numpy())
                values.append(y_test.cpu().numpy())
        return np.concatenate(predictions), np.concatenate(values)

    def plot_losses(self):
        plt.figure(figsize=(10, 5))
        plt.plot(self.train_losses, label='Training Loss')
        plt.plot(self.val_losses, label='Validation Loss')
        plt.title("Training and Validation Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid()
        plt.show()

def calculate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred)**0.5
    r2 = r2_score(y_true, y_pred)
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R2: {r2:.4f}")
    return {'mae': mae, 'rmse': rmse, 'r2': r2}

"""Perfecto, ahora voy a implementar las tres arquitecturas que voy a entretar y comparar que son: RNN, LSTM, y GRU, compatibles todas con la clase Optimization que ya se ha definido previamente."""

class RNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):
        super(RNNModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        out, _ = self.rnn(x, h0.detach())
        out = self.fc(out[:, -1, :])
        return out

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        out, _ = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])
        return out

class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):
        super(GRUModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        out, _ = self.gru(x, h0.detach())
        out = self.fc(out[:, -1, :])
        return out

"""## Resumen de Implementación - Predicción con Deep Learning en Series Temporales

1. **Carga y visualización de datos**  
   - Dataset: `PJME_hourly.csv`, consumo energético horario.
   - Visualización general para detectar estacionalidades (verano/invierno).

2. **Preprocesamiento y generación de características**  
   - Embedding temporal con retardos (`lag_1` a `lag_100`).
   - Variables de calendario: hora, día, mes, día de la semana.
   - Columna binaria `is_holiday` con calendario de EE.UU.

3. **Partición y escalado del dataset**  
   - Separación en entrenamiento, validación y test (80%-10%-10%).
   - Escalado de características con `MinMaxScaler`.

4. **Creación de tensores y DataLoaders con PyTorch**  
   - Conversión a `TensorDataset` y `DataLoader` para entrenamiento en batch.

5. **Definición de arquitecturas**  
   - Modelos implementados: `RNNModel`, `LSTMModel`, `GRUModel`.
   - Cada uno con una capa recurrente + capa completamente conectada (`fc`).

Próximo paso: Entrenamiento generalizado de modelos con función reutilizable.

"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_arr = scaler.fit_transform(X_train)
X_val_arr = scaler.transform(X_val)
X_test_arr = scaler.transform(X_test)

y_train_arr = scaler.fit_transform(y_train)
y_val_arr = scaler.transform(y_val)
y_test_arr = scaler.transform(y_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import plotly.graph_objs as go
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split  # Asegúrate de tener esto importado

# Función de evaluación de métricas
def calculate_metrics(df, col_name="y"):
    result_metrics = {
        'mae': mean_absolute_error(df[col_name], df.prediction),
        'rmse': mean_squared_error(df[col_name], df.prediction) ** 0.5,
        'r2': r2_score(df[col_name], df.prediction)
    }
    print("Mean Absolute Error:      ", result_metrics["mae"])
    print("Root Mean Squared Error:  ", result_metrics["rmse"])
    print("R² Score:                 ", result_metrics["r2"])
    return result_metrics

# Función de inversa del escalado
def inverse_transform(scaler, df, columns):
    for col in columns:
        df[col] = scaler.inverse_transform(df[[col]])
    return df

# Formatear predicciones
def format_predictions(predictions, values, df_test, scaler, col_name="y"):
    vals = np.concatenate(values, axis=0).ravel()
    preds = np.concatenate(predictions, axis=0).ravel()
    df_result = pd.DataFrame(data={col_name: vals, "prediction": preds}, index=df_test.head(len(vals)).index)
    df_result = df_result.sort_index()
    df_result = inverse_transform(scaler, df_result, [col_name, "prediction"])
    return df_result

# Baseline modelo: Regresión lineal
def build_baseline_model(df, test_ratio, target_col):
    X, y = df.drop(columns=[target_col]), df[[target_col]]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)
    model = LinearRegression()
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    result = pd.DataFrame(y_test)
    result["prediction"] = prediction
    result = result.sort_index()
    return result

# Plot predicciones vs reales
def plot_predictions(df_result, df_baseline):
    data = []

    value = go.Scatter(
        x=df_result.index,
        y=df_result.y,
        mode="lines",
        name="Valores reales",
        line=dict(color="rgba(0,0,0, 0.3)"),
    )
    data.append(value)

    baseline = go.Scatter(
        x=df_baseline.index,
        y=df_baseline.prediction,
        mode="lines",
        name="Regresión lineal",
        line={"dash": "dot"},
        opacity=0.8,
    )
    data.append(baseline)

    prediction = go.Scatter(
        x=df_result.index,
        y=df_result.prediction,
        mode="lines",
        name="Predicción modelo DL",
        line={"dash": "dot"},
        opacity=0.8,
    )
    data.append(prediction)

    layout = dict(
        title="Predicciones vs Valores reales",
        xaxis=dict(title="Tiempo"),
        yaxis=dict(title="Consumo (MW)"),
    )

    fig = go.Figure(data=data, layout=layout)
    fig.show()

# Entrenamiento, evaluación y visualización en una sola celda
def General_Settings(model_name, scaler):
    input_dim = len(X_train.columns)
    output_dim = 1
    hidden_dim = 64
    layer_dim = 3
    batch_size = 64
    dropout = 0.2
    n_epochs = 20
    learning_rate = 1e-3
    weight_decay = 1e-6

    model_params = {
        'input_dim': input_dim,
        'hidden_dim': hidden_dim,
        'layer_dim': layer_dim,
        'output_dim': output_dim,
        'dropout_prob': dropout
    }

    model = get_model(model_name, model_params)
    loss_fn = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)
    opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)
    opt.plot_losses()

    predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)

    df_result = format_predictions(predictions, values, X_test, scaler)
    print("\n🔍 Resultados del Modelo Deep Learning:")
    calculate_metrics(df_result)

    return df_result

# Ejecución del pipeline ===
model_name = 'lstm'  # Puedes cambiar a 'rnn' o 'gru'
df_result = General_Settings(model_name, scaler)
df_baseline = build_baseline_model(df_features, 0.2, 'y')  # Asegúrate que df_features esté definido
plot_predictions(df_result, df_baseline)

model_name = 'rnn'
df_result = General_Settings(model_name, scaler)
df_baseline = build_baseline_model(df_features, 0.2, 'y')
plot_predictions(df_result, df_baseline)

# === 🔁 Ejecución del pipeline con GRU ===
model_name = 'gru'
df_result = General_Settings(model_name, scaler)
df_baseline = build_baseline_model(df_features, 0.2, 'y')
plot_predictions(df_result, df_baseline)

print("\n Resultados del Modelo GRU:")
calculate_metrics(df_result)

"""## Conclusión: Reflexiones clave antes de implementar modelos de Deep Learning para Series Temporales

Este análisis responde a las preguntas críticas planteadas antes de implementar redes neuronales (RNN, LSTM, GRU) para predicción de consumo energético.

---

### 1 ¿Qué tipo de dependencias temporales esperas capturar?

- Se observaron **patrones de estacionalidad diarios y semanales** en los datos.
- **Dependencias de medio y largo plazo** son relevantes.
-  **Resultado**: LSTM y GRU superaron a RNN, confirmando que son más adecuadas para este tipo de secuencias.

---

### 2 ¿Qué impacto pueden tener variables exógenas como hora, día o festivos?

- Se incorporaron **features temporales**: hora, día, mes, día de la semana y festivos (`is_holiday`).
-  Mejora clara en las métricas al incluirlas: permiten capturar ciclos de comportamiento humano (laborales vs festivos).

---

### 3 ¿Cuál es la línea base de comparación y qué se espera superar?

- Se usó **regresión lineal** como baseline.
-  LSTM logró mejores métricas:
  - MAE ≈ 645
  - RMSE ≈ 966
  - R² ≈ 0.974
-  Justificación: las redes neuronales mejoran la predicción especialmente en puntos extremos y secuencias largas, donde la regresión lineal falla.

---

### 4 ¿Evidencia de estacionalidad, tendencia o ruido?

- Sí, mediante visualización y análisis de lags:
  - **Consumo cíclico por hora y por estación**.
  - **Ventana de entrada** configurada con 24 lags (1 día).
-  Validación directa del uso de modelos secuenciales.

---

### 5 ¿Estás controlando el sobreajuste?

- Aplicadas técnicas de regularización:
  - `dropout = 0.2`
  - División de datos en `train`, `val`, y `test`
  - Monitoreo de `val_loss` en cada época
- Las curvas muestran convergencia estable sin sobreentrenamiento.

---

### Conclusión final:

> Se validó que **LSTM** es la arquitectura más robusta para este conjunto de datos, mejorando al baseline y generalizando bien gracias al uso de variables calendario, ingeniería de lags y control de sobreajuste. El pipeline es reutilizable y puede escalar a otros dominios con datos similares.

"""