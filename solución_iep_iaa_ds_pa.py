# -*- coding: utf-8 -*-
"""Solución_IEP-IAA-DS_pa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1325DeahsW62syeTKKWrRFP-3ZRtPcYzJ
"""

# Proyecto de Aplicación: Predicción de Fraude en Cuentas Bancarias

# Nombre del estudiante:
# Nombre del docente:
# Asignatura: IE-IAA-DS_pa
# Año: 2025

# ---------------------------------------------
#  Solución al Proyecto de Aplicación
# ---------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Cargar datos
file_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_ML_pa/Base.csv"
data = pd.read_csv(file_path)

# --- 1. Tratamiento de Nulos y Duplicados ---
print("\nValores nulos por columna:")
print(data.isnull().sum())

print("\nDuplicados:", data.duplicated().sum())
data.drop_duplicates(inplace=True)

# --- 2. Análisis Univariante ---
# Identificar tipos de variables
categorical = data.select_dtypes(include='object').columns
discrete = [col for col in data.columns if data[col].nunique() < 10 and col not in categorical]
continuous = [col for col in data.columns if col not in categorical and col not in discrete and col != 'fraud_bool']

# Visualizar distribución de variables continuas
for col in continuous:
    plt.figure(figsize=(6,3))
    sns.histplot(data[col], kde=True)
    plt.title(f"Distribución de {col}")
    plt.show()

# Visualizar categóricas
for col in categorical:
    plt.figure(figsize=(6,3))
    sns.countplot(y=col, data=data, order=data[col].value_counts().index)
    plt.title(f"Frecuencia de {col}")
    plt.show()

# --- 3. Análisis Multivariante ---
# Correlación con la variable objetivo
correlation = data.corr(numeric_only=True)['fraud_bool'].sort_values(ascending=False)
print("\nCorrelación con la variable objetivo:")
print(correlation)

# Eliminar variables altamente correlacionadas entre sí (|corr| > 0.9)
corr_matrix = data.corr(numeric_only=True)
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.9)]
data.drop(columns=to_drop, inplace=True)

# Eliminar columnas no informativas
data.drop(columns=['device_fraud_count'], errors='ignore', inplace=True)

# Convertir categóricas a dummies
data = pd.get_dummies(data, columns=categorical, drop_first=True)

# Eliminar filas nulas si quedan
data.dropna(inplace=True)

# Guardar preprocesado
preprocessed_path = "/content/drive/MyDrive/Colab Notebooks/processed_data/dataset_preprocesado.csv"
data.to_csv(preprocessed_path, index=False)
print("\nDatos preprocesados guardados en:", preprocessed_path)

# --- 4. PCA ---
x = data.drop(columns=['fraud_bool'])
y = data['fraud_bool']

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

# Reducción de dimensionalidad
pca = PCA().fit(x_scaled)
explained_var = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_var)+1), explained_var, marker='o')
plt.axhline(y=0.90, color='r', linestyle='--')
plt.title('Varianza Acumulada por Componentes Principales')
plt.xlabel('Número de Componentes')
plt.ylabel('Varianza Acumulada')
plt.grid(True)
plt.show()

# Elegir componentes que expliquen al menos 90% de la varianza
n_components = np.argmax(explained_var >= 0.90) + 1
print(f"\nNúmero óptimo de componentes: {n_components}")

pca_final = PCA(n_components=n_components)
X_pca = pca_final.fit_transform(x_scaled)

# Guardar dataset con componentes principales
pca_df = pd.DataFrame(X_pca, columns=[f"PC{i+1}" for i in range(n_components)])
pca_df['fraud_bool'] = y.reset_index(drop=True)

pca_path = "/content/drive/MyDrive/Colab Notebooks/processed_data/dataset_pca.csv"
pca_df.to_csv(pca_path, index=False)
print("\nDataset con componentes principales guardado en:", pca_path)

# ---------------------------------------------
#  Aplicación Práctica del Conocimiento
# ---------------------------------------------
# En este proyecto aprendí la importancia de una buena preparación de datos antes de aplicar modelos de Machine Learning.
# Detectar valores nulos, variables redundantes o categorías poco informativas puede afectar significativamente los resultados.
# Además, la aplicación de PCA me permitió reducir dimensionalidad de forma efectiva, algo clave en proyectos con muchos datos.
# Esta experiencia es directamente aplicable en entornos profesionales donde la detección de fraudes financieros requiere
# no solo buenas predicciones, sino también datos limpios y bien entendidos.
# En mi carrera, puedo usar estas técnicas tanto para prevenir fraudes como para realizar segmentaciones y estudios de comportamiento
# basados en datos financieros reales u organizacionales.